{% set version = "2.8.2" %}
# TODO: Add archs 10.0 and 12.0 with CTK 12.8 release

package:
  name: flash-attn-split
  # Strip ".postX" from version number because these just indicate wheel rebuilds and
  # are not relevant to conda builds
  version: {{ version.split('.')[:3] | join('.') }}

source:
  # Use PYPI sdist instead of GitHub because they include a specific revended CUTLASS
  - url: https://pypi.org/packages/source/f/flash-attn/flash_attn-{{ version }}.tar.gz
    sha256: 740a5370f406cbe16155cc6d078ec543a97a137501f32cba89198295e6b80e54
  # Overwrite with a simpler build script that doesn't try to revend pre-compiled binaries
  - path: pyproject.toml
  - path: setup.py

build:
  number: 0
  script: {{ PYTHON }} -m pip install . -vvv --no-deps --no-build-isolation
  script_env:
    # Limit MAX_JOBS in order to prevent runners from crashing
    - MAX_JOBS=4
    - TORCH_CUDA_ARCH_LIST=8.0;8.6;8.9;9.0+PTX
  # Satisfy linter by not having duplicate skip keys
  skip: true  # [cuda_compiler_version == "None" or (not linux)]
  # debugging skips below
  # skip: true  # [py!=313]

requirements:
  build:
    - {{ compiler('c') }}
    - {{ compiler('cxx') }}
    - {{ compiler('cuda') }}
    - {{ stdlib('c') }}
    - ninja
    - pytorch           # [build_platform != target_platform]
    - pytorch * cuda*   # [build_platform != target_platform]
  host:
    - cuda-version {{ cuda_compiler_version }}  # same cuda for host and build
    - cuda-cudart-dev
    - libcublas-dev
    - libcurand-dev
    - libcusolver-dev
    - libcusparse-dev
    - libtorch         # required until pytorch run_exports libtorch
    - pip
    - python
    - pytorch
    - pytorch * cuda*
    - setuptools

outputs:
  - name: flash-attn
    requirements:
      build:
        - {{ compiler('c') }}
        - {{ compiler('cxx') }}
        - {{ compiler('cuda') }}
        - {{ stdlib('c') }}
      host:
        - cuda-version {{ cuda_compiler_version }}  # same cuda for host and build
        - cuda-cudart-dev
        - python
        - libtorch         # required until pytorch run_exports libtorch
        - pytorch
        - pytorch * cuda*
      run:
        - einops
        - python
        - pytorch * cuda*

    files:
      include:
        - 'lib/python*/site-packages/flash_attn/**'
        - 'lib/python*/site-packages/flash_attn-{{ version }}.dist-info/**'
        - 'lib/python*/site-packages/flash_attn_2_cuda.cpython-*.so'

    test:
      imports:
        - flash_attn
      commands:
        - pip check
      requires:
        - pip

  - name: flash-attn-fused-dense
    requirements:
      build:
        - {{ compiler('cxx') }} # needed for DSO checker
        - {{ stdlib('c') }}     # needed for DSO checker
        - {{ compiler('cuda') }}
      host:
        - cuda-version {{ cuda_compiler_version }}  # same cuda for host and build
        - cuda-cudart-dev
        - libcublas-dev
        - python
        - pytorch
        - pytorch * cuda*
      run:
        - python
        - {{ pin_subpackage('flash-attn', exact=True) }}

    files:
      include:
        - 'lib/python*/site-packages/fused_dense_lib.cpython-*.so'

    test:
      imports:
        - flash_attn.ops.fused_dense
      commands:
        - pip check
      requires:
        - pip

  - name: flash-attn-layer-norm
    requirements:
      build:
        - {{ compiler('cxx') }} # needed for DSO checker
        - {{ stdlib('c') }}     # needed for DSO checker
        - {{ compiler('cuda') }}
      host:
        - cuda-version {{ cuda_compiler_version }}  # same cuda for host and build
        - cuda-cudart-dev
        - python
        - pytorch
        - pytorch * cuda*
      run:
        - python
        - {{ pin_subpackage('flash-attn', exact=True) }}

    files:
      include:
        - 'lib/python*/site-packages/dropout_layer_norm.cpython-*.so'

    test:
      imports:
        - flash_attn.ops.layer_norm
      commands:
        - pip check
      requires:
        - pip

about:
  home: https://github.com/Dao-AILab/flash-attention
  summary: 'Flash Attention: Fast and Memory-Efficient Exact Attention'
  license: BSD-3-Clause
  license_file:
    - LICENSE
    - LICENSE_CUTLASS.txt

extra:
  feedstock-name: flash-attn
  recipe-maintainers:
    - carterbox
    - weiji14
