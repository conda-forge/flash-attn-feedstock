schema_version: 1

context:
  version: 2.8.3

recipe:
  name: flash-attn-split
  # Strip ".postX" from version number because these just indicate wheel rebuilds and
  # are not relevant to conda builds
  version: ${{ (version | split('.'))[:3] | join('.') }}

source:
  # Use PYPI sdist instead of GitHub because they include a specific revended CUTLASS
  - url: https://pypi.org/packages/source/f/flash-attn/flash_attn-${{ version }}.tar.gz
    sha256: 1e71dd64a9e0280e0447b8a0c2541bad4bf6ac65bdeaa2f90e51a9e57de0370d
  # Overwrite with a simpler build script that doesn't try to revend pre-compiled binaries
  - path: pyproject.toml
  - path: setup.py

build:
  number: 1
  script:
    env:
      # Limit MAX_JOBS in order to prevent runners from crashing
      MAX_JOBS: "4"
      TORCH_CUDA_ARCH_LIST: 8.0;8.6;8.9;9.0;10.0;12.0+PTX
    content: ${{ PYTHON }} -m pip install . -vvv --no-deps --no-build-isolation
  # Satisfy linter by not having duplicate skip keys
  skip:
    - cuda_compiler_version == "None" or (not linux)
    # debugging skips below
    # - py!=313

outputs:
  - package:
      name: flash-attn
    build:
      files:
        include:
          - lib/python*/site-packages/flash_attn/**
          - lib/python*/site-packages/flash_attn-${{ version }}.dist-info/**
          - lib/python*/site-packages/flash_attn_2_cuda.cpython-*.so
    requirements:
      build:
        - ${{ compiler('c') }}
        - ${{ compiler('cxx') }}
        - ${{ compiler('cuda') }}
        - ${{ stdlib('c') }}
        - ninja
        - if: build_platform != target_platform
          then: pytorch
        - if: build_platform != target_platform
          then: pytorch * cuda*
      host:
        - cuda-version ${{ cuda_compiler_version }}.*
        - cuda-cudart-dev
        - pip
        - python
        - pytorch
        - pytorch * cuda*
      run:
        - einops
        - python
        - pytorch * cuda*
    tests:
      - python:
          imports:
            - flash_attn
          pip_check: true

  - package:
      name: flash-attn-fused-dense
    build:
      files:
        include:
          - lib/python*/site-packages/fused_dense_lib.cpython-*.so
    requirements:
      build:
        - ${{ compiler('c') }}
        - ${{ compiler('cxx') }}
        - ${{ compiler('cuda') }}
        - ${{ stdlib('c') }}
        - ninja
        - if: build_platform != target_platform
          then: pytorch
        - if: build_platform != target_platform
          then: pytorch * cuda*
      host:
        - cuda-version ${{ cuda_compiler_version }}.*
        - cuda-cudart-dev
        - libcublas-dev
        - pip
        - python
        - pytorch
        - pytorch * cuda*
      run:
        - python
        - ${{ pin_subpackage('flash-attn', exact=True) }}
    tests:
      - python:
          imports:
            - flash_attn.ops.fused_dense
          pip_check: true

  - package:
      name: flash-attn-layer-norm
    build:
      files:
        include:
          - lib/python*/site-packages/dropout_layer_norm.cpython-*.so
    requirements:
      build:
        - ${{ compiler('c') }}
        - ${{ compiler('cxx') }}
        - ${{ compiler('cuda') }}
        - ${{ stdlib('c') }}
        - ninja
        - if: build_platform != target_platform
          then: pytorch
        - if: build_platform != target_platform
          then: pytorch * cuda*
      host:
        - cuda-version ${{ cuda_compiler_version }}.*
        - cuda-cudart-dev
        - pip
        - python
        - pytorch
        - pytorch * cuda*
      run:
        - python
        - ${{ pin_subpackage('flash-attn', exact=True) }}
    tests:
      - python:
          imports:
            - flash_attn.ops.layer_norm
          pip_check: true

about:
  homepage: https://github.com/Dao-AILab/flash-attention
  summary: "Flash Attention: Fast and Memory-Efficient Exact Attention"
  license: BSD-3-Clause
  license_file:
    - LICENSE
    - LICENSE_CUTLASS.txt

extra:
  feedstock-name: flash-attn
  recipe-maintainers:
    - carterbox
    - weiji14
